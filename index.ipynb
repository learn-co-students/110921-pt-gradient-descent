{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "9a7141bc16c78c2cbe3925fd27b64699",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Calculus, Cost Function, and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "c9805014c42291e47d89a0e3e45a4010",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![best fit line](visuals/best_fit_line.png)\n",
    "\n",
    "## The best fit line that goes through the scatterplot up above can be generalized in the following equation: $$y = mx + b$$\n",
    "\n",
    "## Of all the possible lines, we can prove why that particular line was chosen using the plot down below:\n",
    "\n",
    "![cost curve](visuals/cost_curve.png)\n",
    "\n",
    "## where RSS is defined as the residual sum of squares:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "RSS &= \\sum_{i=1}^n(actual - expected)^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - \\hat{y})^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - (mx_i + b))^2\n",
    "\\end{align}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "0e0fd36124a7212af7ef3cca03ccfb40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1. What is a more generalized name for the RSS curve above? How is it related to machine learning models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "8bf38bfa82409e1d3eda85c7414d4e6d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "The residual sum of squares curve above is a specific example of a cost curve. \n",
    "\n",
    "When training machine learning models, the goal is to minimize the cost curve.\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3f003514296b2b97f3638e765ecc1d36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2. Would you rather choose a $m$ value of 0.08 or 0.05 from the RSS curve up above?   \n",
    "\n",
    "Assign your answer to the variable `m` in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0682aaf5c7d64e4d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "m = None\n",
    "### BEGIN SOLUTION\n",
    "m = 0.05\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3ac71c4e12448c57",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# PUT ALL WORK FOR THE ABOVE QUESTION ABOVE THIS CELL\n",
    "# THIS UNALTERABLE CELL CONTAINS HIDDEN TESTS\n",
    "\n",
    "# m should be a float, either 0.08 or 0.05\n",
    "assert type(m) == float\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert m == 0.05\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6eba1211850eed30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3. What is the relation between the position on the cost curve, the error, and the slope `m` of the regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ec4bf24ebc6e2dc9ac35b4e54a8b7044",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "It would be better to have a value of 0.05 rather than 0.08 in the cost curve above. The reason for this is that the RSS is lower for the value of 0.05. \n",
    "\n",
    "As m changes values from 0.00 to 0.10, the Residual Sum of Squares is changing.\n",
    "\n",
    "The higher the value of the RSS, the worse the model is performing.\n",
    "\n",
    "So, the minimum RSS value which occurs ~ an m of .05 indicates it's that slope which gives us the best fit for this model\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "7a704cf515e88df1d39d2c7c1c566fb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![gradient descent](visuals/gd.png)\n",
    "\n",
    "### 4. Using the gradient descent visual from above, explain why the distance between each step is getting smaller as more steps occur with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "64b1c28718178fde220c60fa8f7429b4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "The distance between the steps is getting smaller because the slope gradually becomes less and less steep as iterated points for \"m\" get closer to finding the minimum RSS value.\n",
    "\n",
    "Subtracting the derivative - the slope at a given point - thus means subtracting a smaller and smaller value, which translates to smaller and smaller distances between iterated points on the cost curve.  \n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4628880b5f7e4aa70c43174ac3cfe50a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5. What is the purpose of a learning rate in gradient descent? Explain how a very small and a very large learning rate would affect the gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "c3199002b9030e5ea3cb54105e9a13e4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "=== BEGIN MARK SCHEME ===\n",
    "\n",
    "Learning rate is a number that is multiplied by each step that is taken during gradient descent. \n",
    "\n",
    "If the learning rate is smaller, the step sizes will become smaller. \n",
    "\n",
    "If the learning rate is larger, the step sizes will be larger. \n",
    "\n",
    "Learning rate is present in gradient descent to help ensure that an optimal minimum on the cost curve is discovered.\n",
    "\n",
    "=== END MARK SCHEME ==="
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
